---
layout: page
title: Research
date: {{ date }}
modified:
share: false
excerpt:
tags: [research]
image:
  feature:
---

### Student’s experiences with privacy in online learning

![Smithsonian Image]({{ site.url }}/images/1.jpg){: .image-pull-right .image-inside}



I worked with <a href="https://preetimudliar.work//"> Prof. Preeti Mudliar</a> and two other students to learn about students’ experiences with privacy and surveillance in online learning environments. The Covid-19 pandemic disrupted human social interactions. In education, it forced an abrupt shift in teaching and learning to online learning environments. While news reports continue to report on various concerns around online learning ranging from inadequate internet infrastructure, invasive surveillance during exam proctoring, decline in quality of learning, ‘zoom bombing’ of online classrooms etc, we still do not know enough about students’ experiences of this transition and how their learning goals and interactions with peers and instructors changed. We seeked to know more about these classroom experiences through a privacy lens to situate how students respond to and talk about privacy-related concerns in online learning environments. 












<!-- Tensor factorizations are widely used in the literature to model, approximate, and compress high dimensional data for which low-rank assumptions hold.
Many high dimensional real-world data can naturally be expressed as sparse tensors; thereby can be analyzed using tensor factorization.

I am particularly interested in providing high performance parallel sparse tensor factorization kernels to scale such analysis to thousands of processors.
I focus on shared and distributed memory parallel algorithms, tensor data structures for computational and memory efficiency, computational techniques to reduce the amount of computations, and tiling, reordering, partitioning strategies to improve efficiency and scalability of our implementations. In our recent work, we parallalized CANDECOMP/PARAFAC (CP) and Tucker decomposition algorithms, and achieved **up to 700x speedup** using 2048 cores.
We implemented these algorithms in our templated C++ library **HyperTensor**, which uses OpenMP and MPI for shared and distributed memory parallelism (will be available soon). -->

###  Problems in Multi-Task and Multi-Move Rapid Programs

![Smithsonian Image]({{ site.url }}/images/2.jpg){: .image-pull-right .image-inside}



I along with another student worked under the supervision of Prof. Meenakshi d' Souza and Ameena K Ashraf on the Rapid programming language, a proprietary language of ABB to program their industrial robots. We parsed Rapid programs and implemented some rules that check for data-races in multitask and multi-move
programs.
